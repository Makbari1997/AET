{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa321b45-b709-4e22-9c15-5cb25bca7340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/snips...\n",
      "Data is loaded successfully!\n",
      "------------------------------------------------------------------\n",
      "Encoding intent labels...\n",
      "Encoding done successfully!\n",
      "------------------------------------------------------------------\n",
      "Downloading bert-base-uncased\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Download finished successfully!\n",
      "------------------------------------------------------------------\n",
      "Preparing data for bert, it may take a few minutes...\n",
      "Data preparation finished successfully!\n",
      "------------------------------------------------------------------\n",
      "Finetuning of bert is in progress...\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/60\n",
      "291/291 [==============================] - 41s 109ms/step - loss: 0.7487 - f1_m: 0.6810 - categorical_accuracy: 0.8571 - val_loss: 0.3547 - val_f1_m: 0.7352 - val_categorical_accuracy: 0.9154\n",
      "Epoch 2/60\n",
      "291/291 [==============================] - 30s 103ms/step - loss: 0.2695 - f1_m: 0.6660 - categorical_accuracy: 0.9388 - val_loss: 0.1934 - val_f1_m: 0.8136 - val_categorical_accuracy: 0.9577\n",
      "Epoch 3/60\n",
      "291/291 [==============================] - 28s 98ms/step - loss: 0.1654 - f1_m: 0.6312 - categorical_accuracy: 0.9640 - val_loss: 0.1259 - val_f1_m: 0.7939 - val_categorical_accuracy: 0.9712\n",
      "Epoch 4/60\n",
      "291/291 [==============================] - 29s 98ms/step - loss: 0.0984 - f1_m: 0.6858 - categorical_accuracy: 0.9806 - val_loss: 0.0928 - val_f1_m: 0.7838 - val_categorical_accuracy: 0.9827\n",
      "Epoch 5/60\n",
      "291/291 [==============================] - 28s 97ms/step - loss: 0.0670 - f1_m: 0.6813 - categorical_accuracy: 0.9884 - val_loss: 0.0886 - val_f1_m: 0.7900 - val_categorical_accuracy: 0.9865\n",
      "Epoch 6/60\n",
      "291/291 [==============================] - 28s 98ms/step - loss: 0.0550 - f1_m: 0.6875 - categorical_accuracy: 0.9901 - val_loss: 0.0629 - val_f1_m: 0.7211 - val_categorical_accuracy: 0.9885\n",
      "Epoch 7/60\n",
      "291/291 [==============================] - 29s 98ms/step - loss: 0.0393 - f1_m: 0.6679 - categorical_accuracy: 0.9927 - val_loss: 0.0667 - val_f1_m: 0.7436 - val_categorical_accuracy: 0.9885\n",
      "Epoch 8/60\n",
      "291/291 [==============================] - 28s 98ms/step - loss: 0.0330 - f1_m: 0.6963 - categorical_accuracy: 0.9935 - val_loss: 0.0577 - val_f1_m: 0.7703 - val_categorical_accuracy: 0.9885\n",
      "Epoch 9/60\n",
      "291/291 [==============================] - 28s 98ms/step - loss: 0.0273 - f1_m: 0.6773 - categorical_accuracy: 0.9927 - val_loss: 0.0627 - val_f1_m: 0.7767 - val_categorical_accuracy: 0.9885\n",
      "Epoch 10/60\n",
      "291/291 [==============================] - 29s 98ms/step - loss: 0.0240 - f1_m: 0.6895 - categorical_accuracy: 0.9940 - val_loss: 0.0649 - val_f1_m: 0.7904 - val_categorical_accuracy: 0.9885\n",
      "Epoch 11/60\n",
      "291/291 [==============================] - 29s 99ms/step - loss: 0.0192 - f1_m: 0.7251 - categorical_accuracy: 0.9959 - val_loss: 0.0625 - val_f1_m: 0.7607 - val_categorical_accuracy: 0.9904\n",
      "Epoch 12/60\n",
      "291/291 [==============================] - 28s 98ms/step - loss: 0.0160 - f1_m: 0.6842 - categorical_accuracy: 0.9963 - val_loss: 0.0638 - val_f1_m: 0.7685 - val_categorical_accuracy: 0.9885\n",
      "Epoch 13/60\n",
      "291/291 [==============================] - 29s 99ms/step - loss: 0.0150 - f1_m: 0.7388 - categorical_accuracy: 0.9963 - val_loss: 0.0675 - val_f1_m: 0.8052 - val_categorical_accuracy: 0.9865\n",
      "Epoch 14/60\n",
      "291/291 [==============================] - 30s 103ms/step - loss: 0.0105 - f1_m: 0.7659 - categorical_accuracy: 0.9978 - val_loss: 0.0675 - val_f1_m: 0.8183 - val_categorical_accuracy: 0.9885\n",
      "Epoch 15/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 0.0085 - f1_m: 0.7770 - categorical_accuracy: 0.9983 - val_loss: 0.0746 - val_f1_m: 0.8273 - val_categorical_accuracy: 0.9865\n",
      "Epoch 16/60\n",
      "291/291 [==============================] - 32s 108ms/step - loss: 0.0097 - f1_m: 0.7960 - categorical_accuracy: 0.9981 - val_loss: 0.0762 - val_f1_m: 0.8276 - val_categorical_accuracy: 0.9904\n",
      "Epoch 17/60\n",
      "291/291 [==============================] - 30s 104ms/step - loss: 0.0102 - f1_m: 0.7737 - categorical_accuracy: 0.9976 - val_loss: 0.0592 - val_f1_m: 0.8203 - val_categorical_accuracy: 0.9904\n",
      "Epoch 18/60\n",
      "291/291 [==============================] - 32s 109ms/step - loss: 0.0095 - f1_m: 0.8036 - categorical_accuracy: 0.9974 - val_loss: 0.0712 - val_f1_m: 0.8416 - val_categorical_accuracy: 0.9885\n",
      "Epoch 19/60\n",
      "291/291 [==============================] - 30s 104ms/step - loss: 0.0089 - f1_m: 0.8067 - categorical_accuracy: 0.9978 - val_loss: 0.0593 - val_f1_m: 0.8366 - val_categorical_accuracy: 0.9885\n",
      "Epoch 20/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 0.0100 - f1_m: 0.7723 - categorical_accuracy: 0.9987 - val_loss: 0.0572 - val_f1_m: 0.8332 - val_categorical_accuracy: 0.9904\n",
      "Epoch 21/60\n",
      "291/291 [==============================] - 29s 101ms/step - loss: 0.0044 - f1_m: 0.7858 - categorical_accuracy: 0.9996 - val_loss: 0.0569 - val_f1_m: 0.8257 - val_categorical_accuracy: 0.9885\n",
      "Epoch 22/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 0.0048 - f1_m: 0.7525 - categorical_accuracy: 0.9989 - val_loss: 0.0544 - val_f1_m: 0.8061 - val_categorical_accuracy: 0.9885\n",
      "Epoch 23/60\n",
      "291/291 [==============================] - 29s 101ms/step - loss: 0.0083 - f1_m: 0.7951 - categorical_accuracy: 0.9983 - val_loss: 0.0554 - val_f1_m: 0.8123 - val_categorical_accuracy: 0.9885\n",
      "Epoch 24/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 0.0038 - f1_m: 0.7912 - categorical_accuracy: 0.9991 - val_loss: 0.0513 - val_f1_m: 0.8045 - val_categorical_accuracy: 0.9904\n",
      "Epoch 25/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 0.0025 - f1_m: 0.7776 - categorical_accuracy: 1.0000 - val_loss: 0.0544 - val_f1_m: 0.8172 - val_categorical_accuracy: 0.9904\n",
      "Epoch 26/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 0.0029 - f1_m: 0.7863 - categorical_accuracy: 0.9996 - val_loss: 0.0490 - val_f1_m: 0.8083 - val_categorical_accuracy: 0.9904\n",
      "Epoch 27/60\n",
      "291/291 [==============================] - 30s 103ms/step - loss: 0.0024 - f1_m: 0.7923 - categorical_accuracy: 0.9998 - val_loss: 0.0486 - val_f1_m: 0.8130 - val_categorical_accuracy: 0.9904\n",
      "Epoch 28/60\n",
      "291/291 [==============================] - 30s 104ms/step - loss: 0.0020 - f1_m: 0.7826 - categorical_accuracy: 1.0000 - val_loss: 0.0534 - val_f1_m: 0.8067 - val_categorical_accuracy: 0.9904\n",
      "Epoch 29/60\n",
      "291/291 [==============================] - 30s 104ms/step - loss: 0.0089 - f1_m: 0.7779 - categorical_accuracy: 0.9983 - val_loss: 0.0890 - val_f1_m: 0.7970 - val_categorical_accuracy: 0.9827\n",
      "Epoch 30/60\n",
      "291/291 [==============================] - 30s 104ms/step - loss: 0.0046 - f1_m: 0.7683 - categorical_accuracy: 0.9987 - val_loss: 0.1003 - val_f1_m: 0.8172 - val_categorical_accuracy: 0.9827\n",
      "Epoch 31/60\n",
      "291/291 [==============================] - 30s 105ms/step - loss: 0.0021 - f1_m: 0.7675 - categorical_accuracy: 0.9998 - val_loss: 0.0615 - val_f1_m: 0.7787 - val_categorical_accuracy: 0.9885\n",
      "Epoch 32/60\n",
      "291/291 [==============================] - 30s 104ms/step - loss: 0.0039 - f1_m: 0.7251 - categorical_accuracy: 0.9991 - val_loss: 0.0755 - val_f1_m: 0.8300 - val_categorical_accuracy: 0.9885\n",
      "Epoch 33/60\n",
      "291/291 [==============================] - 30s 104ms/step - loss: 0.0034 - f1_m: 0.7836 - categorical_accuracy: 0.9996 - val_loss: 0.0818 - val_f1_m: 0.7961 - val_categorical_accuracy: 0.9885\n",
      "Epoch 34/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 0.0045 - f1_m: 0.7574 - categorical_accuracy: 0.9991 - val_loss: 0.0798 - val_f1_m: 0.7993 - val_categorical_accuracy: 0.9885\n",
      "Epoch 35/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 0.0042 - f1_m: 0.8037 - categorical_accuracy: 0.9996 - val_loss: 0.0710 - val_f1_m: 0.8213 - val_categorical_accuracy: 0.9904\n",
      "Epoch 36/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 0.0014 - f1_m: 0.8000 - categorical_accuracy: 0.9998 - val_loss: 0.0640 - val_f1_m: 0.8123 - val_categorical_accuracy: 0.9885\n",
      "Epoch 37/60\n",
      "291/291 [==============================] - 30s 103ms/step - loss: 0.0011 - f1_m: 0.7954 - categorical_accuracy: 0.9998 - val_loss: 0.0660 - val_f1_m: 0.8167 - val_categorical_accuracy: 0.9904\n",
      "Epoch 38/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 7.9013e-04 - f1_m: 0.7944 - categorical_accuracy: 1.0000 - val_loss: 0.0673 - val_f1_m: 0.8091 - val_categorical_accuracy: 0.9885\n",
      "Epoch 39/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 7.4080e-04 - f1_m: 0.7951 - categorical_accuracy: 1.0000 - val_loss: 0.0652 - val_f1_m: 0.8179 - val_categorical_accuracy: 0.9885\n",
      "Epoch 40/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 9.5209e-04 - f1_m: 0.7896 - categorical_accuracy: 1.0000 - val_loss: 0.0831 - val_f1_m: 0.8098 - val_categorical_accuracy: 0.9904\n",
      "Epoch 41/60\n",
      "291/291 [==============================] - 29s 100ms/step - loss: 0.0050 - f1_m: 0.7803 - categorical_accuracy: 0.9991 - val_loss: 0.0845 - val_f1_m: 0.8023 - val_categorical_accuracy: 0.9904\n",
      "Epoch 42/60\n",
      "291/291 [==============================] - 29s 101ms/step - loss: 0.0016 - f1_m: 0.7946 - categorical_accuracy: 0.9996 - val_loss: 0.0889 - val_f1_m: 0.8135 - val_categorical_accuracy: 0.9904\n",
      "Epoch 43/60\n",
      "291/291 [==============================] - 29s 100ms/step - loss: 5.1404e-04 - f1_m: 0.7983 - categorical_accuracy: 1.0000 - val_loss: 0.0904 - val_f1_m: 0.8065 - val_categorical_accuracy: 0.9885\n",
      "Epoch 44/60\n",
      "291/291 [==============================] - 29s 100ms/step - loss: 0.0079 - f1_m: 0.7131 - categorical_accuracy: 0.9985 - val_loss: 0.0914 - val_f1_m: 0.7551 - val_categorical_accuracy: 0.9885\n",
      "Epoch 45/60\n",
      "291/291 [==============================] - 29s 101ms/step - loss: 0.0022 - f1_m: 0.7294 - categorical_accuracy: 0.9996 - val_loss: 0.0886 - val_f1_m: 0.7734 - val_categorical_accuracy: 0.9885\n",
      "Epoch 46/60\n",
      "291/291 [==============================] - 29s 101ms/step - loss: 0.0022 - f1_m: 0.7327 - categorical_accuracy: 0.9996 - val_loss: 0.0842 - val_f1_m: 0.6979 - val_categorical_accuracy: 0.9885\n",
      "Epoch 47/60\n",
      "291/291 [==============================] - 29s 101ms/step - loss: 0.0045 - f1_m: 0.6229 - categorical_accuracy: 0.9987 - val_loss: 0.0908 - val_f1_m: 0.6293 - val_categorical_accuracy: 0.9865\n",
      "Epoch 48/60\n",
      "291/291 [==============================] - 29s 100ms/step - loss: 9.6019e-04 - f1_m: 0.6981 - categorical_accuracy: 0.9998 - val_loss: 0.0973 - val_f1_m: 0.7857 - val_categorical_accuracy: 0.9885\n",
      "Epoch 49/60\n",
      "291/291 [==============================] - 29s 101ms/step - loss: 4.0641e-04 - f1_m: 0.7457 - categorical_accuracy: 1.0000 - val_loss: 0.0933 - val_f1_m: 0.7996 - val_categorical_accuracy: 0.9885\n",
      "Epoch 50/60\n",
      "291/291 [==============================] - 29s 101ms/step - loss: 3.1313e-04 - f1_m: 0.7649 - categorical_accuracy: 1.0000 - val_loss: 0.0962 - val_f1_m: 0.8004 - val_categorical_accuracy: 0.9885\n",
      "Epoch 51/60\n",
      "291/291 [==============================] - 29s 101ms/step - loss: 2.7544e-04 - f1_m: 0.7794 - categorical_accuracy: 1.0000 - val_loss: 0.0976 - val_f1_m: 0.8075 - val_categorical_accuracy: 0.9885\n",
      "Epoch 52/60\n",
      "291/291 [==============================] - 29s 101ms/step - loss: 2.7909e-04 - f1_m: 0.7831 - categorical_accuracy: 1.0000 - val_loss: 0.1024 - val_f1_m: 0.7930 - val_categorical_accuracy: 0.9865\n",
      "Epoch 53/60\n",
      "291/291 [==============================] - 29s 101ms/step - loss: 0.0021 - f1_m: 0.7795 - categorical_accuracy: 0.9996 - val_loss: 0.0988 - val_f1_m: 0.8019 - val_categorical_accuracy: 0.9885\n",
      "Epoch 54/60\n",
      "291/291 [==============================] - 29s 100ms/step - loss: 2.9828e-04 - f1_m: 0.7873 - categorical_accuracy: 1.0000 - val_loss: 0.0986 - val_f1_m: 0.7987 - val_categorical_accuracy: 0.9885\n",
      "Epoch 55/60\n",
      "291/291 [==============================] - 29s 100ms/step - loss: 2.3605e-04 - f1_m: 0.7998 - categorical_accuracy: 1.0000 - val_loss: 0.1009 - val_f1_m: 0.8031 - val_categorical_accuracy: 0.9885\n",
      "Epoch 56/60\n",
      "291/291 [==============================] - 29s 100ms/step - loss: 1.9387e-04 - f1_m: 0.7987 - categorical_accuracy: 1.0000 - val_loss: 0.1006 - val_f1_m: 0.7988 - val_categorical_accuracy: 0.9865\n",
      "Epoch 57/60\n",
      "291/291 [==============================] - 29s 101ms/step - loss: 1.5103e-04 - f1_m: 0.7983 - categorical_accuracy: 1.0000 - val_loss: 0.1023 - val_f1_m: 0.8067 - val_categorical_accuracy: 0.9865\n",
      "Epoch 58/60\n",
      "291/291 [==============================] - 29s 100ms/step - loss: 1.9767e-04 - f1_m: 0.7977 - categorical_accuracy: 1.0000 - val_loss: 0.0959 - val_f1_m: 0.8063 - val_categorical_accuracy: 0.9865\n",
      "Epoch 59/60\n",
      "291/291 [==============================] - 29s 100ms/step - loss: 1.3980e-04 - f1_m: 0.8036 - categorical_accuracy: 1.0000 - val_loss: 0.0958 - val_f1_m: 0.8121 - val_categorical_accuracy: 0.9865\n",
      "Epoch 60/60\n",
      "291/291 [==============================] - 30s 102ms/step - loss: 3.2170e-04 - f1_m: 0.7967 - categorical_accuracy: 1.0000 - val_loss: 0.0963 - val_f1_m: 0.7873 - val_categorical_accuracy: 0.9885\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Finetuning finished successfully and weights saved to artifacts/snips/bert/\n",
      "------------------------------------------------------------------\n",
      "VAE model creation is in progress...\n",
      "Model is created successfully!\n",
      "------------------------------------------------------------------\n",
      "Training of VAE is in progress...\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 8670.4199\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 3707.6350\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 3324.2441\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 3166.8303\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 3034.9580\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2930.7009\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2914.3425\n",
      "Validation loss: 148.2895\n",
      "Validation loss has improved form inf to 148.2895050048828\n",
      "Saving weights...\n",
      "Time taken: 16.29s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2603.1584\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2426.0425\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2406.4563\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2394.8430\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2383.4983\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2366.2446\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2363.2883\n",
      "Validation loss: 138.3181\n",
      "Validation loss has improved form 148.2895050048828 to 138.31813049316406\n",
      "Saving weights...\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 2287.1077\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2319.6619\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2299.7122\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2286.4836\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2301.0088\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2297.1675\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2296.9570\n",
      "Validation loss: 136.0263\n",
      "Validation loss has improved form 138.31813049316406 to 136.0263214111328\n",
      "Saving weights...\n",
      "Time taken: 9.74s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 2359.0205\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2300.4294\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2289.4836\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2293.2654\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2290.7588\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2286.4026\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2284.3582\n",
      "Validation loss: 137.1867\n",
      "Validation loss has not improved from 136.0263214111328\n",
      "Time taken: 9.51s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 2250.6853\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2252.1118\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2278.3472\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2279.9775\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2279.6387\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2277.6824\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2277.4500\n",
      "Validation loss: 135.6729\n",
      "Validation loss has improved form 136.0263214111328 to 135.67291259765625\n",
      "Saving weights...\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 2146.8325\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2251.7930\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2258.7874\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2254.3489\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2266.9287\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2273.5181\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2271.4568\n",
      "Validation loss: 135.6608\n",
      "Validation loss has improved form 135.67291259765625 to 135.66075134277344\n",
      "Saving weights...\n",
      "Time taken: 8.99s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 2340.5945\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2291.7437\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2289.5632\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2272.8188\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2271.6812\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2270.7083\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2272.3198\n",
      "Validation loss: 135.5751\n",
      "Validation loss has improved form 135.66075134277344 to 135.57513427734375\n",
      "Saving weights...\n",
      "Time taken: 9.22s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 2221.5283\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2283.1077\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2276.4929\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2279.4233\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2270.8298\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2270.7903\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2269.7471\n",
      "Validation loss: 134.6121\n",
      "Validation loss has improved form 135.57513427734375 to 134.612060546875\n",
      "Saving weights...\n",
      "Time taken: 9.32s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 2235.3665\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2280.5952\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2276.1211\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2273.8281\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2272.9546\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2272.5874\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2273.0591\n",
      "Validation loss: 136.0028\n",
      "Validation loss has not improved from 134.612060546875\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 2278.5618\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2286.4485\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2279.3723\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2274.4585\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2274.1143\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2269.9502\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2268.1885\n",
      "Validation loss: 134.0347\n",
      "Validation loss has improved form 134.612060546875 to 134.03466796875\n",
      "Saving weights...\n",
      "Time taken: 10.76s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 2359.1873\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2232.3220\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2242.5225\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2261.8950\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2264.0020\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2265.8533\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2267.2510\n",
      "Validation loss: 133.7780\n",
      "Validation loss has improved form 134.03466796875 to 133.77804565429688\n",
      "Saving weights...\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 2206.2786\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2271.2869\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2276.2551\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2264.9124\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2265.3118\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2267.1165\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2267.9316\n",
      "Validation loss: 132.4724\n",
      "Validation loss has improved form 133.77804565429688 to 132.47242736816406\n",
      "Saving weights...\n",
      "Time taken: 10.57s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 2029.8845\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2239.0857\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2258.1221\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2266.7141\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2270.6931\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2264.3599\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2263.1907\n",
      "Validation loss: 132.5699\n",
      "Validation loss has not improved from 132.47242736816406\n",
      "Time taken: 9.61s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 2151.6040\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2252.8245\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2273.8628\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2264.2737\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2259.8181\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2267.2034\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2266.6506\n",
      "Validation loss: 132.7108\n",
      "Validation loss has not improved from 132.47242736816406\n",
      "Time taken: 10.10s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 2356.3530\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2273.5381\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2266.8979\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2265.4036\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2275.0620\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2273.0291\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2271.1301\n",
      "Validation loss: 136.0695\n",
      "Validation loss has not improved from 132.47242736816406\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 2257.1421\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2242.8198\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2265.0637\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2265.1697\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2269.3315\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2265.1714\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2263.6638\n",
      "Validation loss: 132.7755\n",
      "Validation loss has not improved from 132.47242736816406\n",
      "Time taken: 9.94s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 2431.3455\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2264.6040\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2271.9844\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2270.4695\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2264.8013\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2267.2473\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2266.0691\n",
      "Validation loss: 130.8407\n",
      "Validation loss has improved form 132.47242736816406 to 130.84068298339844\n",
      "Saving weights...\n",
      "Time taken: 11.26s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 2495.8086\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2272.4727\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2269.5146\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2263.4075\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2260.5503\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2266.0747\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2264.1790\n",
      "Validation loss: 135.2604\n",
      "Validation loss has not improved from 130.84068298339844\n",
      "Time taken: 9.29s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 2412.5181\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2250.5461\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2259.3406\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2264.6428\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2260.1360\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2267.0220\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2265.2893\n",
      "Validation loss: 133.6997\n",
      "Validation loss has not improved from 130.84068298339844\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 2357.3306\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2265.4460\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2253.0251\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2251.9253\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2258.7505\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2262.4861\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2261.7676\n",
      "Validation loss: 131.1926\n",
      "Validation loss has not improved from 130.84068298339844\n",
      "Time taken: 9.15s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 2296.6104\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2254.9028\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2294.4805\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2280.5481\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2268.9834\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2267.8765\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2265.0442\n",
      "Validation loss: 131.3323\n",
      "Validation loss has not improved from 130.84068298339844\n",
      "Time taken: 9.47s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 2194.2930\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2267.1860\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2265.1699\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2264.7473\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2265.1606\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2258.3110\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2259.8167\n",
      "Validation loss: 130.3321\n",
      "Validation loss has improved form 130.84068298339844 to 130.3321075439453\n",
      "Saving weights...\n",
      "Time taken: 11.23s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 2243.1772\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2260.6494\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2257.4182\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2304.8567\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2314.4202\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2307.9463\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2307.0171\n",
      "Validation loss: 135.4914\n",
      "Validation loss has not improved from 130.3321075439453\n",
      "Time taken: 9.62s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 2289.6846\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2267.4316\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2265.6550\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2267.3708\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2267.3628\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2269.2871\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2269.1726\n",
      "Validation loss: 135.1382\n",
      "Validation loss has not improved from 130.3321075439453\n",
      "Time taken: 9.17s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 2115.4097\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2276.0811\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2271.5413\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2269.0332\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2262.4536\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2264.7290\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2263.9922\n",
      "Validation loss: 135.2651\n",
      "Validation loss has not improved from 130.3321075439453\n",
      "Time taken: 8.71s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 1998.5880\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2236.5417\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2251.6233\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2256.3425\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2258.6172\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2261.6392\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2261.8196\n",
      "Validation loss: 133.6777\n",
      "Validation loss has not improved from 130.3321075439453\n",
      "Time taken: 8.76s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 2350.0020\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2275.0645\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2276.5327\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2263.0635\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2267.4307\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2268.1897\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2266.6699\n",
      "Validation loss: 134.2070\n",
      "Validation loss has not improved from 130.3321075439453\n",
      "Time taken: 8.67s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 2539.8020\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2298.5874\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2292.6021\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2283.1909\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2278.1189\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2270.8689\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2270.0076\n",
      "Validation loss: 131.7027\n",
      "Validation loss has not improved from 130.3321075439453\n",
      "Time taken: 8.92s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 2215.1433\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2264.0242\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2264.8982\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2266.2046\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2263.8428\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2268.9436\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2265.3430\n",
      "Validation loss: 133.2120\n",
      "Validation loss has not improved from 130.3321075439453\n",
      "Time taken: 9.70s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 2054.4990\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2262.4126\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2264.1763\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2271.4585\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2263.1782\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2263.4790\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2261.3308\n",
      "Validation loss: 134.4406\n",
      "Validation loss has not improved from 130.3321075439453\n",
      "Time taken: 9.72s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 2238.6113\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2254.7332\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2252.3411\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2262.5254\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2267.0662\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2259.0435\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2256.5984\n",
      "Validation loss: 132.8827\n",
      "Validation loss has not improved from 130.3321075439453\n",
      "Time taken: 9.47s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 2187.4375\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2255.8152\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2241.9807\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2240.9333\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2251.1306\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2255.6680\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2255.2126\n",
      "Validation loss: 131.8575\n",
      "Validation loss has not improved from 130.3321075439453\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 2397.8328\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2260.7659\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2268.3445\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2271.1135\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2267.4360\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2258.7239\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2256.9944\n",
      "Validation loss: 129.7532\n",
      "Validation loss has improved form 130.3321075439453 to 129.753173828125\n",
      "Saving weights...\n",
      "Time taken: 11.21s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 2103.9001\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2256.0725\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2249.2585\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2253.6741\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2251.1997\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2257.1218\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2253.1252\n",
      "Validation loss: 129.4248\n",
      "Validation loss has improved form 129.753173828125 to 129.42478942871094\n",
      "Saving weights...\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 2366.2939\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2279.8499\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2271.9946\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2256.1560\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2258.3860\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2259.1162\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2257.9338\n",
      "Validation loss: 128.9669\n",
      "Validation loss has improved form 129.42478942871094 to 128.9668731689453\n",
      "Saving weights...\n",
      "Time taken: 11.08s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 2040.9119\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2241.6445\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2248.5278\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2254.0750\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2254.8098\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2254.4604\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2253.5989\n",
      "Validation loss: 129.8752\n",
      "Validation loss has not improved from 128.9668731689453\n",
      "Time taken: 10.15s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 2256.3965\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2248.0149\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2248.9875\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2248.6042\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2254.8455\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2260.5791\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2261.2415\n",
      "Validation loss: 132.7036\n",
      "Validation loss has not improved from 128.9668731689453\n",
      "Time taken: 8.62s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 2205.8491\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2267.0410\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2266.7297\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2260.2859\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2262.6226\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2257.7046\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2256.7922\n",
      "Validation loss: 130.2370\n",
      "Validation loss has not improved from 128.9668731689453\n",
      "Time taken: 9.63s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 2262.8674\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2276.2383\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2266.3638\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2259.9409\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2262.9287\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2256.8342\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2255.0093\n",
      "Validation loss: 128.7546\n",
      "Validation loss has improved form 128.9668731689453 to 128.754638671875\n",
      "Saving weights...\n",
      "Time taken: 8.85s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 2259.2446\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2252.0898\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2253.8599\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2258.6018\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2259.5918\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2257.5078\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2256.2402\n",
      "Validation loss: 128.9188\n",
      "Validation loss has not improved from 128.754638671875\n",
      "Time taken: 8.42s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 2526.7480\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2256.6743\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2247.2847\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2247.0808\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2252.8066\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2254.7310\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2254.3386\n",
      "Validation loss: 129.1581\n",
      "Validation loss has not improved from 128.754638671875\n",
      "Time taken: 8.65s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 2315.3486\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2267.2815\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2259.0732\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2265.6299\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2257.5388\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2253.5059\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2253.0315\n",
      "Validation loss: 128.0927\n",
      "Validation loss has improved form 128.754638671875 to 128.0926971435547\n",
      "Saving weights...\n",
      "Time taken: 9.57s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 2142.1619\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2264.3411\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2267.3271\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2260.6802\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2259.6946\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2254.6108\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2254.5452\n",
      "Validation loss: 128.3067\n",
      "Validation loss has not improved from 128.0926971435547\n",
      "Time taken: 8.62s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 2118.2097\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2270.2593\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2261.1125\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2268.3269\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2262.8115\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2251.9348\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2250.9380\n",
      "Validation loss: 126.9409\n",
      "Validation loss has improved form 128.0926971435547 to 126.94093322753906\n",
      "Saving weights...\n",
      "Time taken: 9.03s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 2226.3240\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2252.0444\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2242.8059\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2238.8953\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2246.6877\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2257.2102\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2255.2263\n",
      "Validation loss: 127.6839\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.00s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 2148.8005\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2260.7473\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2256.2295\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2259.9170\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2259.0376\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2251.8391\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2251.3193\n",
      "Validation loss: 127.0138\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.17s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 2286.6406\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2239.7966\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2250.7102\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2249.6294\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2250.8696\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2255.7925\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2256.7500\n",
      "Validation loss: 131.7249\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.34s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 2214.6455\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2238.4268\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2252.0352\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2247.2793\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2256.7947\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2255.7395\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2256.7427\n",
      "Validation loss: 128.6855\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 9.23s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 2236.3057\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2269.2388\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2266.3425\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2265.2947\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2262.0820\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2258.3289\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2255.9695\n",
      "Validation loss: 128.9707\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.26s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 2270.2490\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2274.4956\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2268.4219\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2264.1204\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2263.6807\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2256.1685\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2253.6138\n",
      "Validation loss: 128.3312\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.60s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 2212.3069\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2282.2671\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2279.9563\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2268.1855\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2260.1748\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2262.6035\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2260.8582\n",
      "Validation loss: 132.5496\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 9.38s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 2224.2402\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2260.9106\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2245.1995\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2257.5244\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2255.6230\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2255.8384\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2256.0803\n",
      "Validation loss: 128.2840\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.95s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 2083.3838\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2267.7476\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2257.6973\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2246.7695\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2250.6492\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2248.9021\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2246.0713\n",
      "Validation loss: 131.8456\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.35s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 2250.1831\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2257.9187\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2241.8533\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2250.2224\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2244.4072\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2247.4597\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2245.8674\n",
      "Validation loss: 128.3548\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 9.18s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 2109.2832\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2229.5647\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2225.0330\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2238.0557\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2246.5562\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2246.5876\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2246.4590\n",
      "Validation loss: 128.3284\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 7.96s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 2252.4751\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2239.9158\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2239.6484\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2233.4746\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2239.9917\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2240.9497\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2241.3445\n",
      "Validation loss: 128.5052\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 9.09s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 2172.2817\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2216.4224\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2233.1917\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2246.0901\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2244.0903\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2240.0740\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2240.8794\n",
      "Validation loss: 128.1425\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.26s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 2313.4678\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2258.4707\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2235.2336\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2234.1160\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2235.3145\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2238.2615\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2237.9451\n",
      "Validation loss: 127.2130\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 9.26s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 2453.9275\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2234.3328\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2230.7615\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2235.0796\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2234.3508\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2238.5642\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2238.2986\n",
      "Validation loss: 129.0294\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.17s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 2211.6804\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2241.6807\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2232.5518\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2235.2561\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2248.9592\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2263.7791\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2263.3401\n",
      "Validation loss: 133.7377\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 9.65s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 2170.4734\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2284.1145\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2283.4458\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2264.2180\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2260.9324\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2254.3311\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2252.9104\n",
      "Validation loss: 129.9610\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 10.01s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 2013.8866\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2256.5564\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2240.3420\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2233.7839\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2232.9185\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2236.2458\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2238.2739\n",
      "Validation loss: 128.7422\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.88s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 2058.6912\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2250.7324\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2242.5549\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2235.5527\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2240.6321\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2236.6306\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2236.4026\n",
      "Validation loss: 128.8980\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.05s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 2250.1943\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2217.7847\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2227.9761\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2229.2280\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2235.0701\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2237.8699\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2238.5149\n",
      "Validation loss: 128.4190\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.48s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 2176.2258\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2241.6997\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2238.9727\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2244.9729\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2238.3225\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2236.6870\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2236.9817\n",
      "Validation loss: 128.7354\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 9.59s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 2351.2666\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2249.6890\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2247.7000\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2243.2251\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2242.0469\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2238.1492\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2235.6763\n",
      "Validation loss: 128.1203\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.68s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 2130.6323\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2223.6555\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2232.2212\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2233.8000\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2231.7607\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2234.6968\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2235.2476\n",
      "Validation loss: 127.9972\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 9.57s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 2201.9880\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2205.9626\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2232.0493\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2230.5291\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2234.2056\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2235.0435\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2235.2148\n",
      "Validation loss: 127.6160\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.85s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 2105.4897\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2192.4685\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2213.4333\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2230.2622\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2227.4514\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2236.4155\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2235.5552\n",
      "Validation loss: 128.1761\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 9.10s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 2050.1440\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2236.4600\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2237.1582\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2240.5706\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2240.4106\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2238.4421\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2238.1223\n",
      "Validation loss: 127.8587\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.16s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 2127.6348\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2267.6350\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2241.5164\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2231.9790\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2235.4619\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2236.4512\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2233.4907\n",
      "Validation loss: 127.7558\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 9.94s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 2532.5200\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2234.9270\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2237.3711\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2249.1650\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2237.9094\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2238.2783\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2236.3130\n",
      "Validation loss: 127.3488\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 9.21s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 2156.7664\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2213.4246\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2222.1843\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2224.6326\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2234.4587\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2235.2002\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2235.5801\n",
      "Validation loss: 128.3841\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 2137.7510\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2256.0840\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2236.7102\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2235.7402\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2235.0259\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2231.9998\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2230.8921\n",
      "Validation loss: 127.4850\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.69s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 2368.4495\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2216.3081\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2247.2029\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2238.8108\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2235.4966\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2235.8254\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2234.9148\n",
      "Validation loss: 128.7333\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.64s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 2233.7964\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2235.3076\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2233.9805\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2237.7202\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2231.2900\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2233.1665\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2229.5967\n",
      "Validation loss: 127.7916\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.89s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 2384.5063\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2223.5867\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2219.6362\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2215.0554\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2219.0791\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2224.4636\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2224.5776\n",
      "Validation loss: 127.6525\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.16s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 2220.1353\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2237.5894\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2243.8770\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2237.1646\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2229.3284\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2222.7578\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2222.6716\n",
      "Validation loss: 127.4453\n",
      "Validation loss has not improved from 126.94093322753906\n",
      "Time taken: 8.49s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 2195.2319\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2228.0205\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2221.9712\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2218.8469\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2215.7019\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2222.3269\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2223.4341\n",
      "Validation loss: 126.7490\n",
      "Validation loss has improved form 126.94093322753906 to 126.74900817871094\n",
      "Saving weights...\n",
      "Time taken: 9.04s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 2209.7732\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2217.2029\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2223.9883\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2223.3376\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2221.0784\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2223.3804\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2220.4297\n",
      "Validation loss: 126.0992\n",
      "Validation loss has improved form 126.74900817871094 to 126.09919738769531\n",
      "Saving weights...\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 2098.0977\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2233.4028\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2213.5083\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2217.0042\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2217.5713\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2218.5867\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2217.8298\n",
      "Validation loss: 126.5754\n",
      "Validation loss has not improved from 126.09919738769531\n",
      "Time taken: 8.54s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 2081.7200\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2221.7400\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2234.3499\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2234.2815\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2230.8821\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2228.0122\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2224.7971\n",
      "Validation loss: 126.7468\n",
      "Validation loss has not improved from 126.09919738769531\n",
      "Time taken: 8.03s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 2142.6064\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2213.1821\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2215.5889\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2219.4084\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2220.1091\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2216.1277\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2215.2068\n",
      "Validation loss: 127.0120\n",
      "Validation loss has not improved from 126.09919738769531\n",
      "Time taken: 7.85s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 2088.0947\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2219.6382\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2222.2490\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2221.2222\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2210.6558\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2209.7449\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2210.5361\n",
      "Validation loss: 126.1301\n",
      "Validation loss has not improved from 126.09919738769531\n",
      "Time taken: 8.19s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 2399.9326\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2201.5317\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2204.8528\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2200.0552\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2205.1902\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2205.5137\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2205.1514\n",
      "Validation loss: 127.5869\n",
      "Validation loss has not improved from 126.09919738769531\n",
      "Time taken: 8.66s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 2181.9631\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2203.8105\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2210.4719\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2195.8684\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2201.8174\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2205.7849\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2205.7756\n",
      "Validation loss: 126.2556\n",
      "Validation loss has not improved from 126.09919738769531\n",
      "Time taken: 8.58s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 2138.3469\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2227.9197\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2208.9915\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2206.7969\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2202.3064\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2203.7083\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2201.6196\n",
      "Validation loss: 125.6192\n",
      "Validation loss has improved form 126.09919738769531 to 125.61920166015625\n",
      "Saving weights...\n",
      "Time taken: 9.52s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 2342.9236\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2198.2090\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2209.9275\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2206.7534\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2204.5786\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2202.8474\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2201.2822\n",
      "Validation loss: 126.2592\n",
      "Validation loss has not improved from 125.61920166015625\n",
      "Time taken: 8.44s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 2539.0928\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2224.4138\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2206.9878\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2203.1904\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2202.6978\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2200.0439\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2200.7754\n",
      "Validation loss: 126.2773\n",
      "Validation loss has not improved from 125.61920166015625\n",
      "Time taken: 9.23s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 2354.7500\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2220.3206\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2211.2676\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2205.8921\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2202.6257\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2201.9246\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2201.8118\n",
      "Validation loss: 125.5347\n",
      "Validation loss has improved form 125.61920166015625 to 125.5346908569336\n",
      "Saving weights...\n",
      "Time taken: 8.77s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 2094.7964\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2188.1079\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2192.5525\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2205.9663\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2199.5659\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2204.1079\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2203.4048\n",
      "Validation loss: 125.7037\n",
      "Validation loss has not improved from 125.5346908569336\n",
      "Time taken: 7.84s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 2065.6079\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2203.2195\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2193.1665\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2203.5591\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2199.7397\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2202.3486\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2200.4370\n",
      "Validation loss: 125.3062\n",
      "Validation loss has improved form 125.5346908569336 to 125.30615997314453\n",
      "Saving weights...\n",
      "Time taken: 9.27s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 2197.0366\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2195.4185\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2194.1326\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2194.2249\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2197.9656\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2202.5564\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2201.3657\n",
      "Validation loss: 125.3143\n",
      "Validation loss has not improved from 125.30615997314453\n",
      "Time taken: 9.34s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 2134.8218\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2199.6831\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2186.0903\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2204.0659\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2203.4280\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2200.7417\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2198.6062\n",
      "Validation loss: 126.0838\n",
      "Validation loss has not improved from 125.30615997314453\n",
      "Time taken: 8.35s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 2051.1797\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2175.2090\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2199.1606\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2194.8743\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2196.9399\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2196.2112\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2197.2217\n",
      "Validation loss: 125.3925\n",
      "Validation loss has not improved from 125.30615997314453\n",
      "Time taken: 8.75s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 2160.8726\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2184.2893\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2190.8020\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2193.7119\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2199.1909\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2196.9668\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2196.6130\n",
      "Validation loss: 125.7925\n",
      "Validation loss has not improved from 125.30615997314453\n",
      "Time taken: 8.42s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 2143.3013\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2196.5679\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2196.5110\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2196.2754\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2193.3237\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2197.7417\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2196.4683\n",
      "Validation loss: 125.4114\n",
      "Validation loss has not improved from 125.30615997314453\n",
      "Time taken: 8.26s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 2110.0312\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2187.3049\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2189.8440\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2189.7600\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2198.7625\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2199.7178\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2198.2412\n",
      "Validation loss: 125.7643\n",
      "Validation loss has not improved from 125.30615997314453\n",
      "Time taken: 9.01s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 2037.2909\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2191.9138\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2185.0396\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2184.3669\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2191.5581\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2194.3477\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2195.7358\n",
      "Validation loss: 125.4259\n",
      "Validation loss has not improved from 125.30615997314453\n",
      "Time taken: 9.12s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 2038.2170\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 50: 2173.7651\n",
      "Seen so far: 816 samples\n",
      "Training loss (for one batch) at step 100: 2168.5771\n",
      "Seen so far: 1616 samples\n",
      "Training loss (for one batch) at step 150: 2178.0427\n",
      "Seen so far: 2416 samples\n",
      "Training loss (for one batch) at step 200: 2186.3496\n",
      "Seen so far: 3216 samples\n",
      "Training loss (for one batch) at step 250: 2186.8040\n",
      "Seen so far: 4016 samples\n",
      "Training loss over epoch: 2189.9067\n",
      "Validation loss: 124.7064\n",
      "Validation loss has improved form 125.30615997314453 to 124.70636749267578\n",
      "Saving weights...\n",
      "Time taken: 9.08s\n",
      "Training is done and weights saved to artifacts/snips/vae/vae.h5\n",
      "------------------------------------------------------------------\n",
      "Calculating train and dev loss for visualization...\n",
      "You can use figures in artifacts/snips to decide what threshold should be used.\n",
      "------------------------------------------------------------------\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).classifier.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).classifier.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.weight\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.token_type_embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.pooler.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.pooler.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).classifier.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).classifier.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.weight\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.token_type_embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.pooler.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.pooler.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).classifier.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).classifier.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.weight\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.token_type_embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.pooler.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.pooler.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).classifier.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).classifier.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.weight\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.token_type_embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.pooler.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.pooler.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.beta\n"
     ]
    }
   ],
   "source": [
    "! python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f20d4ff-d7dd-4f19-b683-52c6cffa1e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/snips...\n",
      "Data is loaded successfully!\n",
      "------------------------------------------------------------------\n",
      "Encoding intent labels...\n",
      "Encoding done successfully!\n",
      "------------------------------------------------------------------\n",
      "Downloading bert-base-uncased\n",
      "2025-05-12 21:25:35.390937: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-12 21:25:35.478036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14449 MB memory:  -> device: 0, name: NVIDIA RTX 2000 Ada Generation, pci bus id: 0000:c2:00.0, compute capability: 8.9\n",
      "2025-05-12 21:25:35.895363: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Download finished successfully!\n",
      "------------------------------------------------------------------\n",
      "Preparing data for bert, it may take a few minutes...\n",
      "Data preparation finished successfully!\n",
      "------------------------------------------------------------------\n",
      "Loading bert weights from artifacts/snips/bert/\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "------------------------------------------------------------------\n",
      "VAE model creation is in progress...\n",
      "Model was created successfully and weights were loaded from artifacts/snips/vae/vae.h5.\n",
      "------------------------------------------------------------------\n",
      "------------------------------------------------------------------\n",
      "Using EVT for VAE losses only\n",
      "EVT Results (threshold = 0.1048):\n",
      "  Binary F1 Macro: 0.9030\n",
      "  Binary F1 Micro: 0.9081\n",
      "  AUC-ROC: 0.5000\n",
      "\n",
      "Best Fixed Threshold Results (threshold = 0.1000):\n",
      "  Binary F1 Macro: 0.8993\n",
      "  Binary F1 Micro: 0.9049\n",
      "\n",
      "EVT Threshold 95% Confidence Interval: [0.0879, 0.1197]\n",
      "Using EVT threshold: 0.1048\n",
      "----------------------------------\n",
      "Multi class macro f1: 0.7665\n",
      "Multi class micro f1: 0.9049\n",
      "\n",
      "\n",
      "Binary class macro f1: 0.9030\n",
      "Binary class micro f1: 0.9081\n",
      "AUC-ROC: 0.9202\n",
      "------------------------------------------------------------------\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).classifier.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).classifier.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.weight\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.token_type_embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.pooler.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.pooler.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).classifier.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).classifier.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.weight\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.token_type_embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.pooler.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.pooler.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).classifier.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).classifier.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.weight\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.token_type_embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.pooler.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.pooler.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).classifier.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).classifier.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.weight\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.token_type_embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.pooler.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.pooler.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.intermediate.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.intermediate.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.bert_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.10.attention.dense_output.LayerNorm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.query.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.query.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.key.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.key.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.self_attention.value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).bert.encoder.layer.11.attention.dense_output.LayerNorm.beta\n"
     ]
    }
   ],
   "source": [
    "! python predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec3162-613c-4811-a30d-e9c539123712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
